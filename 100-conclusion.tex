\chapter{Conclusion}

During this thesis, we explored how Deep Learning applied to Computer Vision could bring value to a video platform. We started with a simple idea: extract semantic information from video content in order to improve user experience (through content categorization and searchability for instance) and content suggestion and personnalization. This naturally lead to two main parts: extracting information from videos, and creating a recommender system. We settled on some information that would be interesting: activity classification could help categorize the content and choose a fitting thumbnail when showing search results, and face recognition for similar purposes. In a second time, we would analyze the challenges presented with a recommender system in our domain and propose a model.

The systems presented here are used in production or about to be, bringing value as improved user experience. This allows us to conclude on several points. First and foremost, deep learning is able to deliver on our data type and domain, which corroborates the large amount of testimonies of businesses positively impacted by the usage of deep learning.

\paragraph{Semantic activity information} could be extracted and leveraged as see in Section \ref{actionclf}. We could analyze the videos fairly easily and reliably enough for the way we want to exploit them. We were able to develop a model, evaluating several options including randomly initialized models, pretrained models and data augmentation. We show that data augmentation and pretrained models were able to significantly increase our accuracy up to a point where the model could be good enough for some non critical labeling, improving the user experience.

\paragraph{Negative training samples} were leveraged with standard softmax  classifiers in order to use  so that our face recognition classifier becomes more robust to unknown people (Chapter \ref{chap:fr}). We first set the face recognition problem as a standard metric learning problem, showed that it does not scale for our problem. Since the people we want to recognized are known at training time, we simplified our problem down to a standard classifier in need an out-of-domain rejection ability. We compared several loss functions in order to implement this rejection capability: standard cross-entropy, cross-entropy with an additional out-of-domain class, cross-entropy augmented with a logit regularizer on distractors, and cross-entropy augmented with a maximum entropy objective for distractors. Each model improved on the previous one, the maximum entropy loss being the best we tried. We also showcased that all those models exhibit satisfying calibration, and the rejection threshold could be set by the production engineers in order to specify the error rate tolerance by trading recall for precision.

\paragraph{Recommender system} were our subject of study in Chapter \ref{chap:recsys}, when we have no user ratings but user browsing histories instead. We compared about fifty models trained on different feature subsets. It was shown that providing tag embeddings pretrained with Word2Vec lead to significant gains, making it one of the most important features, followed by learnable uploader embeddings. This model needs to be A/B compared with the current system in production, based on manual heuristics.

\paragraph{Torchélie} is a framework based on pytorch that was introduced in Chapter \ref{chap:tch}, underpinning all experiments and industrial developments done during the thesis. We presented how its design, based on deltas and patches, diverged from current famous frameworks. We implemented in it various contributions, including the proposed \ac{VQ} layers. The pertinence of those design principles were supported by examples exhibiting how a standard resnet can be patched to produce its variants, and how a conditional GAN algorithm can be gradually patched to produce the Pix2Pix algorithm then Pix2PixHD.

Those achievements were accomplished thanks to \textbf{three datasets} that we put together, explained in Section \ref{sec:hexadata}. HActions is composed of frames extracted from videos, sorted into 13 classes, representing various activities. HFaces is our face recognition dataset, containing 8938 identities with an average of 50 pictures per identity. We ran our experiments on the 105 most popular identities, using the rest as distractors that have to be rejected by the classifier. Finally, we leveraged HHistory, the browsing history of our premium users for our recommendation engine. It contains several metadata, 3673 uploaders, 140k videos and 136k users. We searched for ways to grow those dataset in a principled ways in Section \ref{sec:pr-dataset}.

Besides those industry focused goals, we explored the metric learning framework and proposed the \textbf{Threshold-Softmax}, a new loss function able to learn from negative examples (in Section \ref{sec:tsm}). The threshold-Softmax proposes to learn face embeddings fitting a cone with an absolute maximum angle, rather than imposing angular margins between classes. Negative samples are forced in the negative space: outside of the regions allocated for the positive classes. We experimented this loss on MS1Mv2 and compared it to the \ac{SotA} ArcFace. The Threshold-Softmax is competitive but not always superior to ArcFace, but presents the ability to learn from unlabeled negative samples (unknown people not belonging to any positive class), halving the error rate in our tests on LFW and FGLFW.

During the ongoing exploration on generative models based on the \ac{VQVAE}, we proposed to improve the efficiency and control of the quantization layer thanks to our \textbf{expiring codebook} in Section \ref{sec:vqexpir}. An expiration mechanism is added to the codebook. When a code has not been used for more than a fixed number of training iterations, it is resampled to an input data point. This threshold is an hyper parameter that allows controlling the entropy of assignments. Experiments showed that this algorithm lead to better training dynamics that consistently outperformed the original \ac{VQ} algorithm and trained faster.

We used this to build a \textbf{face swap} model (Section \ref{sec:facegen}) based on a \ac{VQ} bottleneck. A face picture is encoded, constrained with a \ac{VQ} bottleneck, and decoded back. We also provide a learnable embedding of the identity label to the decoder. In order to produce an accurate reconstruction despite the bottleneck, the model has to learn as much as possible about facial geometry and store that knowledge in the embedding. The bottleneck can be used to transport the remaining information: colors, lighting, pose, etc. We showed that this model produces crisp pictures, and that the face swap is successful, actually fooling a face recognition system.

\paragraph{Some future work} remain to be done and questions to be answered. We mainly need to put all those systems together and test whether the metadata extracted from videos are able to improve the recommender system. The impact on the face recognition system of the samples generated from the face swap algorithm is still to be evaluated. This \ac{VQVAE} system can be compared to a \ac{GAN} based model, as they are famous for the great image quality they produce. The recommender system has to be evaluated in production and A/B tested to assess its real world performance. Torchélie needs to find a community to maintain it and keep it up to date as the needs for deep learning grow too much for a single developer. Most importantly, Torchélie must now get up to date with modern training recipes (involving Mixup, CutMix, TrivialAugment, etc) while mitigating the cost they incur, often needing longer training schedules or smaller batch sizes, if we want to keep our extreme applicability focus.

\paragraph{The perspectives} that were opened by this work are numerous.

There could be many interesting ways to categorize videos. Instead of building big datasets for classifiers with human defined label sets, we could envision training a model with unsupervised techniques on our videos and use small datasets for few shot classification, allowing faster iterations on ways to categorize the data with respect to their business impact. Models like \ac{CLIP} \cite{openaiclip} could also be interesting for the zero-shot classification ability they provide, but our domain has no such model built for use, and no dataset of captioned images; could we develop a \ac{CLIP}-like model with constrained data resources, or what would it take to fine tune CLIP?

The Threshold-Softmax can be extended to integrate ArcFace's margins as well, maybe able to improve on it. Experiments should be lead to understand precisely how ArcFace utilizes its latent space and how Threshold-Softmax organizes the negative samples in the latent space. Those insights could help improving both, and maybe Metric Learning in general. The same detailed experiments could be done for the four losses we explored for negative training (vanilla cross-entropy, with a Discriminator class, penalizing the logits, maximizing the entropy).

Our face swap model lacks the ability to generalize to new identities, which is very important for data augmentation. Some work paved the ways in reversing \acp{GAN}, which could be an inspiration to reverse the proposed architecture. This would allow editing pictures in latent space, and open a wide range of new possibilities.

Some of it is priority work as the industrial interests are highly convergent with this exploration.
